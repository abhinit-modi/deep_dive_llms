{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df536a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c2606a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3, 12, 6])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 6\n",
    "sequence = torch.rand((5, 8, 3, 12))  # Tensor: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "windows = sequence.unfold(1, window_size, 2)\n",
    "windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7f1e7461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4]) torch.Size([1, 2, 3, 4]) torch.Size([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 3\n",
    "num_heads = 2\n",
    "head_dim = 4\n",
    "hidden_size = num_heads * head_dim\n",
    "\n",
    "window_size = 2\n",
    "padding = window_size // 2\n",
    "\n",
    "queries = torch.rand((batch_size, num_heads, seq_len, head_dim))\n",
    "keys = torch.rand((batch_size, num_heads, seq_len, head_dim))\n",
    "values = torch.rand((batch_size, num_heads, seq_len, head_dim))\n",
    "\n",
    "print(queries.shape, keys.shape, values.shape)\n",
    "values\n",
    "\n",
    "queries_ = queries.clone().detach()\n",
    "keys_ = keys.clone().detach()\n",
    "values_ = values.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7c4cf7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.7016, 0.8530],\n",
       "           [0.8243, 0.5465]],\n",
       "\n",
       "          [[0.0369, 0.3384],\n",
       "           [0.4608, 0.1522]],\n",
       "\n",
       "          [[0.2582, 0.2979],\n",
       "           [0.0515, 0.4759]]],\n",
       "\n",
       "\n",
       "         [[[0.5817, 0.8130],\n",
       "           [0.1351, 0.8070]],\n",
       "\n",
       "          [[0.7353, 0.8495],\n",
       "           [0.3326, 0.1671]],\n",
       "\n",
       "          [[0.2150, 0.2406],\n",
       "           [0.0405, 0.5801]]]]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.reshape((batch_size, num_heads, seq_len, head_dim // 2 , 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9eb09378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 2, 4]) torch.Size([1, 2, 3, 2, 4])\n",
      "torch.Size([1, 2, 3, 2])\n",
      "torch.Size([1, 2, 3, 2])\n",
      "torch.Size([1, 2, 3, 2])\n",
      "torch.Size([1, 3, 2, 4])\n",
      "torch.Size([1, 3, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4173, 0.0515, 0.2000, 0.2083, 0.4375, 0.0428, 0.7168, 0.5457],\n",
       "         [0.4546, 0.3947, 0.7048, 0.4549, 0.3701, 0.1302, 0.9216, 0.4152],\n",
       "         [0.5426, 0.7919, 0.8697, 0.3831, 0.1972, 0.5028, 0.6689, 0.4262]]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "keys = F.pad(keys, (0, 0, padding, padding), \"constant\", 0)\n",
    "values = F.pad(values, (0, 0, padding, padding), \"constant\", 0)\n",
    "\n",
    "# Impl 1\n",
    "# Permute to get (batch_size, num_heads, seq_len, window_size, head_dim)\n",
    "# # This aligns window_size as the second-to-last dimension for einsum\n",
    "\n",
    "keys_windowed = keys.unfold(2, window_size, 1)[:, :, :seq_len, :, :].permute(0, 1, 2, 4, 3)\n",
    "values_windowed = values.unfold(2, window_size, 1)[:, :, :seq_len, :, :].permute(0, 1, 2, 4, 3)\n",
    "print(keys_windowed.shape, values_windowed.shape)\n",
    "\n",
    "scores = torch.einsum('bnsh,bnswh->bnsw', queries, keys_windowed)\n",
    "print(scores.shape)\n",
    "\n",
    "scores = scores / (head_dim ** 0.5)\n",
    "print(scores.shape)\n",
    "\n",
    "attention = F.softmax(scores, dim=-1)\n",
    "print(attention.shape)\n",
    "\n",
    "context = torch.einsum('bnsw,bnswh->bsnh', attention, values_windowed)\n",
    "print(context.shape)\n",
    "\n",
    "context = context.reshape(batch_size, seq_len, hidden_size)\n",
    "print(context.shape)\n",
    "\n",
    "context_1 = context.clone().detach()\n",
    "context_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "031d4508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4173, 0.0515, 0.2000, 0.2083, 0.4375, 0.0428, 0.7168, 0.5457],\n",
       "         [0.4546, 0.3947, 0.7048, 0.4549, 0.3701, 0.1302, 0.9216, 0.4152],\n",
       "         [0.5426, 0.7919, 0.8697, 0.3831, 0.1972, 0.5028, 0.6689, 0.4262]]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impl 2\n",
    "# Initialize context tensors\n",
    "keys_ = F.pad(keys_, (0, 0, padding, padding), \"constant\", 0)\n",
    "values_ = F.pad(values_, (0, 0, padding, padding), \"constant\", 0)\n",
    "\n",
    "context = torch.zeros_like(queries_)\n",
    "\n",
    "# Compute attention for each sliding window\n",
    "for i in range(seq_len):\n",
    "    # Determine the start and end of the window\n",
    "    start = i\n",
    "    end = i + window_size\n",
    "    \n",
    "    # Compute scores\n",
    "    # (batch_size, num_heads, seq_length, head_dim)\n",
    "    scores = torch.matmul(queries_[:, :, i:i+1, :], keys_[:, :, start:end, :].transpose(-2, -1))\n",
    "    scores = scores / (head_dim ** 0.5)\n",
    "    attention = F.softmax(scores, dim=-1)\n",
    "    # print(attention)\n",
    "    \n",
    "    \n",
    "    # Apply attention to values and add to context\n",
    "    context[:, :, i:i+1, :] += torch.matmul(attention, values_[:, :, start:end, :])\n",
    "\n",
    "# Reshape context to (batch_size, seq_length, num_heads * head_dim)\n",
    "context = context.permute(0, 2, 1, 3).reshape(batch_size, seq_len, hidden_size)\n",
    "print(context.shape)\n",
    "\n",
    "context_2 = context.clone().detach()\n",
    "context_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c7176fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(context_1, context_2, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d7130882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Einsum Implementation ---\n",
      "Context shape (einsum): torch.Size([2, 5, 32])\n",
      "\n",
      "--- Original Loop Implementation (for comparison) ---\n",
      "Context shape (loop): torch.Size([2, 5, 32])\n",
      "\n",
      "Results from einsum and loop implementations are close!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sliding_window_attention_einsum(queries, keys, values, window_size, padding, head_dim, hidden_size):\n",
    "    batch_size = queries.shape[0]\n",
    "    num_heads = queries.shape[1]\n",
    "    seq_len = queries.shape[2]\n",
    "    \n",
    "    padded_keys = F.pad(keys, (0, 0, padding, padding), \"constant\", 0) # Pads last two dims: (0,0 for head_dim, padding,padding for seq_dim)\n",
    "    padded_values = F.pad(values, (0, 0, padding, padding), \"constant\", 0)\n",
    "\n",
    "    unfolded_keys = padded_keys.unfold(2, window_size, 1)\n",
    "    unfolded_values = padded_values.unfold(2, window_size, 1)\n",
    "\n",
    "    K_windows = unfolded_keys[:, :, :seq_len, :, :]  # Shape: (B, N, S, Dk, W)\n",
    "    V_windows = unfolded_values[:, :, :seq_len, :, :]  # Shape: (B, N, S, Dk, W)\n",
    "\n",
    "    # K_windows = K_windows.permute(0, 1, 2, 4, 3)\n",
    "    # V_windows = V_windows.permute(0, 1, 2, 4, 3)\n",
    "\n",
    "    scores = torch.einsum('bnsd,bnsdw->bnsw', queries, K_windows)\n",
    "\n",
    "    scores = scores / (head_dim ** 0.5)\n",
    "    attention_weights = F.softmax(scores, dim=-1) # Softmax over W (window_size dimension)\n",
    "                                               # Shape: (B, N, S, W)\n",
    "\n",
    "    context = torch.einsum('bnsw,bnsdw->bnsd', attention_weights, V_windows)\n",
    "\n",
    "    context = context.permute(0, 2, 1, 3).reshape(batch_size, seq_len, hidden_size)\n",
    "    \n",
    "    return context\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example Usage (matching the variable names from the problem if possible)\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    num_heads = 4\n",
    "    head_dim = 8\n",
    "    hidden_size = num_heads * head_dim # 32\n",
    "    window_size = 3\n",
    "    \n",
    "    # For the padding to match the original example's implied structure:\n",
    "    # The original code's loop is:\n",
    "    # for i in range(seq_len):\n",
    "    #   start = i\n",
    "    #   end = i + window_size\n",
    "    #   scores = torch.matmul(queries[:, :, i:i+1, :], keys_padded[:, :, start:end, :].transpose(-2, -1))\n",
    "    # This means the window is purely forward or centered around the query 'i' depending on how 'padding' relates to 'window_size'.\n",
    "    # If padding = window_size -1 (as it seems to be in many such implementations for causal/local attention)\n",
    "    # this creates a \"look-ahead\" style window of size `window_size` starting at `i` in the padded sequence.\n",
    "    \n",
    "    # Let's use the specific padding value from the original snippet if it was fixed,\n",
    "    # or define it based on common practice if it was a variable `padding`.\n",
    "    # The prompt used a variable `padding`. Let's assume it's `window_size - 1` for this example,\n",
    "    # which is a common setup for ensuring windows can be formed at sequence boundaries.\n",
    "    # Or, for centered windows, padding = window_size // 2 (if window_size is odd)\n",
    "    # The original problem statement had `padding` as a variable. So we should accept it.\n",
    "    # For this test, let's use padding = window_size // 2 for a somewhat centered window.\n",
    "    # If window_size = 3, padding = 1.\n",
    "    # If we use the same padding as the prompt `padding = window_size - 1 = 2` for `window_size = 3`\n",
    "    example_padding = window_size -1 # This seems to be a common pattern for such sliding windows.\n",
    "                                  # If window_size = 3, then padding = 2.\n",
    "                                  # Padded seq_len = 5 + 2*2 = 9.\n",
    "                                  # Keys: p p k0 k1 k2 k3 k4 p p\n",
    "                                  # q0 att to (p,p,k0) (using start=0, end=3 in original loop)\n",
    "                                  # This interpretation for start=0 in padded sequence might be off.\n",
    "                                  # Original loop: start=i, end=i+window_size.\n",
    "                                  # If padding is on *original keys* not *padded_keys_indices*\n",
    "                                  # This needs to be consistent with original for comparison.\n",
    "\n",
    "    # The provided snippet's `padding` is an input to F.pad.\n",
    "    # Let's test with values that make sense.\n",
    "    # If window_size = 3, a common padding is 1 on each side (total window covers q_i-1, q_i, q_i+1)\n",
    "    padding_amount_for_test = window_size // 2 # e.g., 1 for window_size=3\n",
    "\n",
    "    queries_tensor = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "    keys_tensor = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "    values_tensor = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "    print(\"--- Einsum Implementation ---\")\n",
    "    context_einsum = sliding_window_attention_einsum(\n",
    "        queries_tensor, keys_tensor, values_tensor,\n",
    "        window_size, padding_amount_for_test, head_dim, hidden_size\n",
    "    )\n",
    "    print(\"Context shape (einsum):\", context_einsum.shape)\n",
    "\n",
    "\n",
    "    # For comparison, let's try to replicate the original loop logic\n",
    "    # The original code snippet used a variable `padding`.\n",
    "    # We must use the SAME `padding_amount_for_test` for a fair comparison.\n",
    "    \n",
    "    print(\"\\n--- Original Loop Implementation (for comparison) ---\")\n",
    "    # Original Impl (adapted for standalone test)\n",
    "    # Note: The 'padding' variable in the original snippet is the amount of padding on each side.\n",
    "    \n",
    "    # Initialize context tensors\n",
    "    _keys = F.pad(keys_tensor, (0, 0, padding_amount_for_test, padding_amount_for_test), \"constant\", 0)\n",
    "    _values = F.pad(values_tensor, (0, 0, padding_amount_for_test, padding_amount_for_test), \"constant\", 0)\n",
    "    \n",
    "    _context_loop = torch.zeros_like(queries_tensor) # (B, N, S, Dk)\n",
    "    \n",
    "    # Compute attention for each sliding window\n",
    "    for i in range(seq_len):\n",
    "        # Determine the start and end of the window in the padded keys/values\n",
    "        start = i \n",
    "        end = i + window_size\n",
    "        \n",
    "        # Slice the query\n",
    "        q_i = queries_tensor[:, :, i:i+1, :] # (B, N, 1, Dk)\n",
    "        \n",
    "        # Slice the window from padded keys and values\n",
    "        k_window = _keys[:, :, start:end, :]     # (B, N, W, Dk)\n",
    "        v_window = _values[:, :, start:end, :]   # (B, N, W, Dk)\n",
    "        \n",
    "        # Compute scores\n",
    "        # (B, N, 1, Dk) @ (B, N, Dk, W) -> (B, N, 1, W)\n",
    "        scores_loop = torch.matmul(q_i, k_window.transpose(-2, -1))\n",
    "        scores_loop = scores_loop / (head_dim ** 0.5)\n",
    "        attention_loop = F.softmax(scores_loop, dim=-1)\n",
    "        \n",
    "        # Apply attention to values and add to context\n",
    "        # (B, N, 1, W) @ (B, N, W, Dk) -> (B, N, 1, Dk)\n",
    "        attended_values_loop = torch.matmul(attention_loop, v_window)\n",
    "        _context_loop[:, :, i:i+1, :] = attended_values_loop # Use '=' since it's 0-initialized, original had += but context_i was new\n",
    "\n",
    "    # Reshape context to (batch_size, seq_len, num_heads * head_dim)\n",
    "    _context_loop_final = _context_loop.permute(0, 2, 1, 3).reshape(batch_size, seq_len, hidden_size)\n",
    "    print(\"Context shape (loop):\", _context_loop_final.shape)\n",
    "\n",
    "    # Check if results are close (they should be identical if logic is same)\n",
    "    if torch.allclose(context_einsum, _context_loop_final, atol=1e-6):\n",
    "        print(\"\\nResults from einsum and loop implementations are close!\")\n",
    "    else:\n",
    "        print(\"\\nResults from einsum and loop implementations DIFFER!\")\n",
    "        # print(\"Einsum context head:\", context_einsum.view(-1)[:10])\n",
    "        # print(\"Loop context head:\", _context_loop_final.view(-1)[:10])\n",
    "        # print(\"Difference:\", torch.abs(context_einsum - _context_loop_final).max())\n",
    "\n",
    "    # Recreate the specific output of the original problem for context_2\n",
    "    # context = context_einsum # Assuming this is the primary output\n",
    "    # print(context.shape) # Already printed\n",
    "    # context_2 = context.clone().detach()\n",
    "    # print(context_2) # This would print the tensor values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5d5da86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5]) tensor([0.0000, 0.2000, 0.4000, 0.6000, 0.8000])\n",
      "tensor([1.0000, 0.3981, 0.1585, 0.0631, 0.0251])\n",
      "torch.Size([5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=10\n",
    "period=100\n",
    "context_size=5\n",
    "\n",
    "exps = (1./dim) * torch.arange(0, (dim-1), 2)\n",
    "print(exps.shape, exps)\n",
    "freqs = (1./torch.pow(period, exps))\n",
    "print(freqs)\n",
    "\n",
    "token_indexes = torch.arange(0, context_size)\n",
    "    \n",
    "# TODO: compute the matrix thetas\n",
    "thetas = torch.outer(token_indexes, freqs)\n",
    "print(thetas.shape)\n",
    "\n",
    "# TODO: create the rotation matrix\n",
    "rotation_matrix = torch.polar(torch.ones_like(thetas), thetas)\n",
    "rotation_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9f89bb54",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [1, 10, 2] at index 2 does not match the shape of the indexed tensor [1, 10, 10] at index 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[285], line 35\u001b[0m\n\u001b[1;32m     22\u001b[0m topk_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[[\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m     23\u001b[0m          [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m],\n\u001b[1;32m     24\u001b[0m          [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m9\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m          [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m     31\u001b[0m          [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m5\u001b[39m]]])\n\u001b[1;32m     34\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(topk_indices \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mout\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# x[torch.where(topk_indices == 3)]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# batch_szie x seq_len x experts\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [1, 10, 2] at index 2 does not match the shape of the indexed tensor [1, 10, 10] at index 2"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[-0.8772, -0.6420, -0.1764,  0.4584, -0.8136, -0.6509, -0.5871,\n",
    "           0.6453,  0.0991, -0.3950],\n",
    "         [ 0.3687, -0.9497, -0.9017,  0.1877,  0.9783, -0.1219,  0.8722,\n",
    "          -0.3780,  0.3587,  0.3795],\n",
    "         [ 0.4127,  1.5541,  0.7300, -0.1090, -0.2371,  0.0809,  0.1396,\n",
    "          -0.6884,  0.0311,  0.8754],\n",
    "         [ 0.2966, -0.3919, -0.5396, -0.6623, -0.0381,  0.8342,  1.2686,\n",
    "          -1.3032,  0.4697,  0.4133],\n",
    "         [ 0.2843, -0.2664, -0.3312, -0.4801,  0.3916,  1.0380,  1.3515,\n",
    "          -0.5189, -0.1152, -0.0456],\n",
    "         [-0.1839, -0.3232, -0.6245,  0.4363, -0.3063,  0.4891, -0.0958,\n",
    "          -0.2570,  0.8761, -0.4627],\n",
    "         [-0.1320, -0.3170, -0.6218,  0.4533, -0.3246,  0.4891, -0.0982,\n",
    "          -0.2880,  0.8828, -0.4444],\n",
    "         [-0.1295, -0.3561, -0.6142,  0.4811, -0.3424,  0.4857, -0.1472,\n",
    "          -0.2484,  0.8728, -0.4723],\n",
    "         [-0.1699, -0.3207, -0.5689,  0.4708, -0.3790,  0.4455, -0.2404,\n",
    "          -0.2106,  0.8703, -0.4348],\n",
    "         [-0.1862, -0.3174, -0.5680,  0.4516, -0.3676,  0.4528, -0.2310,\n",
    "          -0.2155,  0.8513, -0.4210]]])\n",
    "\n",
    "topk_indices = torch.tensor([[[7, 3],\n",
    "         [4, 6],\n",
    "         [1, 9],\n",
    "         [6, 5],\n",
    "         [6, 5],\n",
    "         [8, 5],\n",
    "         [8, 5],\n",
    "         [8, 5],\n",
    "         [8, 3],\n",
    "         [8, 5]]])\n",
    "\n",
    "\n",
    "out = torch.where(topk_indices == 3, True, False)\n",
    "x[out]\n",
    "\n",
    "# x[torch.where(topk_indices == 3)]\n",
    "# batch_szie x seq_len x experts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8fa94ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 3]),)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3,4,5])\n",
    "torch.where(x%2==0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
