{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f38d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"facebook/opt-125m\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2c3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (lora_adapter_1): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (lora_adapter_1): Linear(in_features=768, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (lora_adapter_1): Linear(in_features=16, out_features=768, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (lora_adapter_1): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (lora_adapter_1): Linear(in_features=768, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (lora_adapter_1): Linear(in_features=16, out_features=768, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    task_type=\"CAUSAL_LM\")\n",
    "\n",
    "# Define the vectors you want to tune (you might have to explicitly specify the layers)\n",
    "# For example, you can specify the layers like this:\n",
    "# lora_config.target_modules = [\"q_proj\", \"v_proj\"]  # Example for a model with these layers\n",
    "# Note: The actual layer names depend on the model architecture.\n",
    "\n",
    "\n",
    "# What other types of task_types are supported in LoraConfig constructor?\n",
    "# Answer: The `task_type` can be \"CAUSAL_LM\", \"SEQ_2_SEQ_LM\", \"TEXT_CLASSIFICATION\", \"TOKEN_CLASSIFICATION\", etc.\n",
    "# Here we are using \"CAUSAL_LM\" for a causal language model like OPT\n",
    "## How do the modules change based on the task_type?\n",
    "# The modules that are targeted for LoRA adaptation can vary based on the task type.\n",
    "# For example, in a text classification task, you might target different layers than in a causal language modeling task.\n",
    "\n",
    "# By what fraction does LoRA reduce the number of trainable parameters?\n",
    "# Answer: LoRA can significantly reduce the number of trainable parameters, often by a factor of 10x or more, depending on the model size and the rank `r` specified in the configuration.\n",
    "# This is because LoRA introduces low-rank matrices that are much smaller than the original weight matrices.\n",
    "\n",
    "\n",
    "model.add_adapter(adapter_name=\"lora_adapter_1\", adapter_config=lora_config)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5297a73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abmodi/dev/LLM-bootcamp-homework/Homework4/.venv/lib/python3.9/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/Users/abmodi/dev/LLM-bootcamp-homework/Homework4/.venv/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForCausalLM(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "          (embed_positions): lora.Embedding(\n",
       "            (base_layer): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (lora_adapter_1): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict()\n",
       "            (lora_B): ModuleDict()\n",
       "            (lora_embedding_A): ParameterDict(  (lora_adapter_1): Parameter containing: [torch.FloatTensor of size 16x2050])\n",
       "            (lora_embedding_B): ParameterDict(  (lora_adapter_1): Parameter containing: [torch.FloatTensor of size 768x16])\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x OPTDecoderLayer(\n",
       "              (self_attn): OPTSdpaAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (spanish_adapter): Identity()\n",
       "                    (french_adapter): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (spanish_adapter): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    (french_adapter): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (spanish_adapter): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    (french_adapter): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (spanish_adapter): Identity()\n",
       "                    (french_adapter): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (spanish_adapter): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    (french_adapter): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (spanish_adapter): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    (french_adapter): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # target_modules=['k_proj']\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config, adapter_name=\"spanish_adapter\")\n",
    "peft_model.add_adapter(\n",
    "    adapter_name=\"french_adapter\",\n",
    "    peft_config=lora_config\n",
    ")\n",
    "peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f1486d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "spanish_dataset = load_dataset('andreamorgar/spanish_poetry', split='train')\n",
    "french_dataset = load_dataset('Abirate/french_book_reviews', split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc45bc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book_title': 'Le Démon de la Colline aux Loups',\n",
       " 'author': 'Dimitri Rouchon-Borie',\n",
       " 'reader_review': 'Ce n\\'est pas le premier roman à aborder les thèmes lourds de l\\'inceste et de l\\'enfance martyre, mais il le fait avec une audace et un brio incomparables qui rendent ce livre marquant dans une vie de lecteur. On y sent à quel point l\\'auteur n\\'a pas cherché à \"faire quelque chose\", on ne sent jamais l\\'intention, on sent juste l\\'urgence, incandescente, à raconter la vérité d\\'un homme maltraité par la vie au point de dire à la nuit «\\xa0 tu ne me feras pas peur j\\'ai plus de noir que toi dans mon enfance\\xa0».',\n",
       " 'rating': 5.0,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "190c4d0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3261040518.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    .filter(lambda x: x['content'] != None)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def preprocess_spanish_data(examples):\n",
    "    return examples\n",
    "        .filter(lambda x: x['content'] != None)\n",
    "        .map(lambda x: tokenizer(x['content'], max_length=128, truncation=True, padding='max_length'),\n",
    "        remove_columns=spanish_dataset.column_names\n",
    "    )\n",
    "\n",
    "def preprocess_french_data(examples):\n",
    "    return examples\n",
    "        .filter(lambda x : x['reader_review'] != None)\n",
    "        .map(lambda x: tokenizer(x['reader_review'], max_length=128, truncation=True, padding='max_length'),\n",
    "        remove_columns=french_dataset.column_names\n",
    "    )\n",
    "\n",
    "preprocessed_spanish_data = preprocess_spanish_data(spanish_dataset)\n",
    "preprocessed_french_data = preprocess_french_data(french_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a20aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af545bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1926' max='1926' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1926/1926 08:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.580400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1926, training_loss=3.623957902347806, metrics={'train_runtime': 537.0508, 'train_samples_per_second': 28.662, 'train_steps_per_second': 3.586, 'total_flos': 1019462653181952.0, 'train_loss': 3.623957902347806, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./spanish_model\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "peft_model.set_adapter(\"spanish_adapter\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_spanish_data,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543d16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3624' max='3624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3624/3624 16:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.575600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.526700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.492600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.488400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.482200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3624, training_loss=3.50653835742942, metrics={'train_runtime': 1017.8681, 'train_samples_per_second': 28.465, 'train_steps_per_second': 3.56, 'total_flos': 1918918398836736.0, 'train_loss': 3.50653835742942, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./french_model\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "peft_model.set_adapter(\"french_adapter\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_french_data,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da932ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "quantized_base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "peft_model = get_peft_model(quantized_base_model, lora_config, adapter_name=\"spanish_adapter\")\n",
    "peft_model.add_adapter(\n",
    "    adapter_name=\"french_adapter\",\n",
    "    peft_config=lora_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc134d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs, max_length=128)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3530cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Meta-Llama-3-8B'\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac6835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from data_loading import DataLoader\n",
    "from model_loading import ModelLoader\n",
    "from data_processing import DataCollatorForSupervisedDataset, DataProcessor\n",
    "\n",
    "GROUP_SIZE_RATIO = 1/4\n",
    "\n",
    "data = DataLoader.get_data()\n",
    "print(f\"Loaded {len(data)} examples from the dataset.\")\n",
    "print(f\"First example: {data[0]}\")\n",
    "\n",
    "model_loader = ModelLoader(group_size_ratio=GROUP_SIZE_RATIO)\n",
    "model, tokenizer = model_loader.load_and_prepare_model()\n",
    "print(\"Model and tokenizer loaded and prepared.\")\n",
    "\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer, GROUP_SIZE_RATIO)\n",
    "\n",
    "data_processor = DataProcessor(tokenizer)\n",
    "tokenized_data = data_processor.transform(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97bd3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abmodi/dev/LLM-bootcamp-homework/Homework4/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from data_loading import DataLoader\n",
    "from model_loading import ModelLoader\n",
    "from data_processing import DataCollatorForSupervisedDataset, DataProcessor\n",
    "\n",
    "GROUP_SIZE_RATIO = 1/4\n",
    "\n",
    "data = DataLoader.get_data()\n",
    "model_loader = ModelLoader(group_size_ratio=GROUP_SIZE_RATIO)\n",
    "model_id = model_loader.model_id\n",
    "config = model_loader.get_config(model_id)\n",
    "scaling_factor = model_loader.scaling_factor\n",
    "\n",
    "tokenizer = model_loader.load_tokenizer(model_id, scaling_factor, config)\n",
    "\n",
    "data_processor = DataProcessor(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba99143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset...\n",
      "Dataset transformed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file': None,\n",
       " 'output': 'Answer: The paper mentions several directions for improvement:\\n\\n1. Sampling more video frames to capture high-frequency motion information better. The current model may still miss some fine-grained details due to sparse sampling.\\n\\n2. Using more powerful pre-trained models that specifically model objects and actions. The current patch-level features have limitations in complex object-level reasoning. \\n\\n3. Combining different pre-trained models to achieve more general reasoning ability. Each model has its own strengths and weaknesses, so an ensemble may perform better.\\n\\n4. Handling longer videos that are more commonly seen in real-world applications. The current model focuses on efficiency but may still struggle with hours-long videos.\\n\\n5. Exploring spatial selection modules to reduce computation for high-resolution videos. The current model only uses region selection, but spatial selection may be useful for very complex videos.\\n\\nIn summary, sampling strategies, using specialized pre-trained models, and further improving efficiency for extremely long videos are potential directions to enhance the proposed model.',\n",
       " 'instruction': 'Below is a paper. Memorize the paper and answer my question after the paper.\\n The paper begins. \\n Abstract\\nTo build Video Question Answering VideoQA) systems in daily activities,  seeking capable of assisting humans answers from long-form videos with diverse and complex Existing multi-modal VQA models achieve events is a must. promising performance video images or  short clips; on of large-scale   multi- especially with the recent success However; when extending these meth- modal pre-training: challenges arise: long-form videos; ods On the to new sampling strategy is com- one hand, a dense video using putationally prohibitive. On the other hand, methods rely ing on sparse sampling struggle in scenarios where multi- event and multi-granularity visual reasoning are required. new model named Multi- In this work; we introduce modal Iterative Spatial-temporal Transformer (MIST) to adapt pre-trained models for long-form VideoQA: better MIST decomposes traditional dense spatial Specifically; temporal self-attention into cascaded segment and region selection modules that adaptively select frames and image regions that are closely relevant to the question itself Vi- sual concepts at different granularities are then processed efficiently through an attention module. In addition,  MIST iteratively conducts selection and attention over multiple to support reasoning over multiple events. The exper- layers imental results on four VideoQA datasets, including AGQA, MIST achieves NExT-QA, STAR, and Env-QA, show that state-of-the-art performance and is superior at efficiency The code is available at qithub. com; showlab_ mist.\\nMain challenges of long-form VideoQA. The ques- Figure tions for long-form VideoQA usually involve multi-event; multi- grained, and causality reasoning:\\nfunctions, the systems   should be able to understand and seek the answer from long-form videos with diverse events about users activities.\\nCompared to understanding and reasoning over   short videos, many unique challenges arise when the duration of Fig: the video increases, as shown in 1: 1) Multi-event rea- soning: The long-form videos usually record much more The questions about these videos thus naturally re- events_ the systems to perform complex temporal reasoning Iquire e.g,, multi-event reasoning (Ql in Fig: 1), causality (Q3), 2) Interactions among different granularities of visual etc. concepts. The questions of short-clip videos usually involve the interactions of objects or actions that happened simulta - neously, while questions for long-form videos could involve more complex interactions of objects, relations, and events Fig: across different events; e.g,, Q2 in 1_\\n1. Introduction\\nOne of the ultimate goals of Video Question Answering (VideoQA) systems is to assist people in solving problems in everyday life [13,27,41], e.g , helping users find some- reminding them thing; they and assisting them what did, accomplishing complex tasks, etc_ To achieve such while\\nvision-language methods [2,7, 10, 24,29, 31 Current over images or short clips span- 32,51,52] excel at QA they ning several seconds. In other words, excel at learn- ing multi-modal  correspondences single cap between tion with one or few Their tremendous progress events is   fueled by 1) pre-training large- these years over on short-clip-language scale image-language [22,37,38] and\\nCurrently at Google Brain_\\nshown in Fig: 2 MIST comes from simple finding that for long-form VideoQA, it is not necessary t0 consider the like details of all events in a video, what dense self-attention over all patches do_ The model only needs to consider the general content of all events and focuses on the de question-related Thus, MIST de- tails of a few events_ composes dense joint spatial-temporal self-attention into question-conditioned cascade segment and region selection along spatial-temporal  self-attention module with over multi-modal multi-grained features. The cascade selection reduces the computation cost and benefits the performance by focusing 0 the question-related segments and regions. over segments and image patches, bet- The self-attention ter captures interactions among different granularities of vi- In addition, through iteratively conducting sual concepts_ selection and self-attention, MIST can reason over multiple events and better perform temporal and causal reasoning:\\nexperiments on  several  VideoQA datasets We conduct with relatively longer videos, AGQA 14], NExT-QA [44] STAR [42], and Env-QA [11], with an average video du- The experimental results 12s to 44s_ ration varies from perfor approach   achieves show that state-of-the-art our mance. Further ablation studies verify the effectiveness of components. Moreover; quantitative and qualitative key the results also show that our method provides higher efficiency and reasonable evidence for answering questions.\\nDiagrammatic illustration of MIST Figure It revises 2 layer standard  spatial-temporal self-attention into two modules a cascade selection module that dynamically eliminates question- irrelevant image regions, and a self-attention layer reasoning over multi-modal multi-grained visual concepts. The proposed mod- ules further iterate multiple times t0 reason over different events \\n2. Related Work\\ndatasets [2,33], and 2) end-to-end multi-modal Transform- align- ers [1-3,10,37,40], which is superior at learning the ments between images with texts.\\nVideo question   answering: Video Question Answer ing is one typical type of vision-language task studied for Some datasets [20, 47] focus on short clips many years. about daily human activities, eg-, sports, household work MovieQA [39], Some others,  such TVQA [25], etc. as long videos cropped and Social-IQ [50], mainly focus on from movies or TV series for evaluating the understanding of the plot and social interactions, where subtitles play an essential role_ [11,14,42,44] aim to evaluate Recently; more complex spatial-temporal reasoning long-form over videos, e.g , causality, sequential order; etc Current works achieve promising results on the first two types of bench- marks, while struggle on the last one, which is our focus_\\nthese multi-modal Transformers  rely However; on the increasing dense self-attention with the computation cost exponentially over time especially when adapting to long- To make the dense self-attention computation- form videos_ ally feasible in processing videos, almost all current state- sample-based of-the-art pre-trained Transformers are sparse methods, e.g;, [2,40] only sample 3 or 4 frames per video regardless of its length. If we simply adapt these pre-trained models to long-form videos with the same sampling strat- egy, there will be a domain gap between the pre-training and downstream  VideoQA tasks. In pre-training, the sparsely sampled frames of a short video depict a coherent action they likely to be random shots for part of events while are long video Recently, some early attempts process the in video hierarchically [5], which splits the video into several segments and performs QA oly o aggregated segment- efficiency issue, but is still level features_ It can ease the hard to capture complex interactions among multi-grained concepts.   Thus, leveraging the advantages of models pre- images Or short videos and addressing the chal- trained from lenges of long-form VideoQA is worth exploring:\\nterms of methodology, early-stage works  proposed In various LSTM or Graph Neural Network-based models to capture cross-modal [28,35, 54] o motion-appearance in- 45] integrates graph teraction [12,23]. One recent work modeling into Transformers to explicitly capture the objects and their relations in videos_ In addition, with the great vision-language Transformers, many success of pre-trained works [2,10,40] directly fine-tune the pre-trained model on simple yet ef- downstream VideoQA tasks_ [5] proposes fective fine-tuning strategy to hierarchically process videos with pre-trained Transformers_\\nIn this paper; a new model named Multi- we propose modal Iterative Spatial-temporal TTransformer (MIST), as\\nCompared to previous works, this paper is an early at-\\ntempt to specifically focus 0n the challenges of long-form VideoQA for Transformers-based methods. Specifically, we revise the self-attention mechanism to better perform multi-event, multi-grained visual concepts reasoning:\\nwhere y is the predicted answer chosen from the candidate answers (i.e,, answer vocabulary Or provides choices), de- noted and 0 is the set of trainable parameters of as A, VideoQA model F.\\nTransferring pre-trained models downstream to Many transfer  pre-trained tasks. works   try vision- to language  Transformers, such CLIP [37], into down- as object tasks, detection image [15], stream e.g , gen video-text eration [36], and retrieval [9, 32, 48, 53] CLIPAClip [32] proposes various aggregation methods for pooling; Transformer; CLIP features, eg;, to better mean represent a video. CLIP2 Video [9] proposes a temporal dif- ference block to better capture motion information. Sim above  methods, we preserve the strengths of ilar to the pre-trained models and improve their weaknesses on down- long- stream tasks, but this on another one, works focus form VideoQA where the main focus is on multi-event and multi-granularity reasoning\\nFig: pipeline of our proposed Multi 3 In we present the Transformer; MIST. Modal Iterative Spatial-temporal MIST answers the question in three steps: 1) utilize pre trained model to extract the input features, 2) iteratively per- form self-attention over a selected set of features to perform multi-event reasoning, 3) predict the answer based on the obtained video, question, and answer features_\\n3.1. Input Representation\\nExisting vision-language Transformers are good at rep resenting images To adapt them to handle or short clips. the long-form video, we first split the video into K uniform length segments,  where each segment contains T frames In addition, each frame is divide into N patches_ Note that_ for the simplicity of notation, the [CLS] token for image patches and frames are counted in N and T.\\nLong-form video modeling: With the success of great short-term video understanding in recent years, some pio neer works [8,43] have started to focus on long-form video They modeling for action recognition Or localization tasks increasing the efficiency f processing long mainly focus on video features. [8] proposes short-term feature extraction and long-term memory mechanisms that can eliminate the during need for processing redundant video frames training: [30] proposes to replace of the video with compact parts audio cues to succinctly summarize dynamic audio events and are cheap to process. [16] introduces structured multi to improve effi scale temporal decoder for self-attention kciency: The above methods utilize the natural characteris tics of videos to reduce the computation. In contrast, this paper considers the characteristics of QA tasks to use the guide to reduce computation. question as\\nThe vision-language Transformer; like CLIP All-in-one with frozen parameters, extracts patch-level features of all cK }, where zk z2 RTxNxD {2l , segments, x is the feature of k-th segment; where D is the dimension of each patch-level feature. The patch-level visual token fea- tures will be used to obtain frame and segment features in following modules the Since the segment features are sep arately extracted, to indicate their temporal positions in the whole video, we add position embedding Pt € {o(i)li [0, K T]} for each token with their frame index:\\nFor the text part, the question is tokenized as a sequence of words, and then fed into the vision-language Transformer word-level features Xw {W1, } , where W1 to WM get corresponds to [CLS] and WM are words in question. W2 .\\nIterative Attention.  Many existing works [4,6, 17, 34] improving computation efficiency. are for Some of them propose similar iterative attention mechanisms t0 ours. [34] image classification  model to itera- a recurrent proposes tively attending on a sequence of regions at high resolution Perceiver 17] revises  self-attention in Transformer to an asymmetric attention to iteratively distill inputs into a tight feature, allowing it to handle large inputs. TimeSformer [ proposes various self-attention schemes for video classifica- tion models to separately apply temporal and spatial atten- Our model differs in utilizing multi-modal correspon- tion dence (i.e , vision and question) to iterative attention_ guide\\n3.2. Iterative Spatial-Temporal Attention Layer\\nThe Iterative   Spatial-Temporal Attention layer (ISTA aims to iteratively select the segments and regions among questions and then perform long video conditioned on Specifically, multi-event   reasoning over   selected ones ISTA contains three steps: segment selection, region selec - tion, and spatial-temporal self-attention; as shown in Fig:\\nSegment Selection: Given a of image patch features set T, we calculate the features of segments and the question segments by perform- then select the patch features of Topk ing cross-modal temporal attention and differentiable top-k selection.\\n3. Method\\nSpecifically, to perform temporal attention, the frame features are first obtained by pooling the features in spa- tial dimension: the t-th frame feature in k-th segment is calculated as fk pool(zk,1,xk,2, where wt,n in- xt, N), dicates n-th patch at t-th frame of k-th segment. Then, the are obtained by pooling frames features segment features\\nThe goal of a VideoQA task is to predict the answer for a given video V and a question q, formulated as follows:\\nFigure 3_ Architecture of   MIST MIST first divides video into several segments and utilizes the pre-trained (PT) video encoder to extract the feature of each one. Then, MIST iteratively performs self-attention over a selected set of features to reason over multiple events_ Finally, it predicts the answer by comparing the combination of video and question features with answer candidate features_ Note that the \"PT Video Encoder\" in the figure can also be image-based encoders_\\nX {wk K_1\\' and question features g, we first perform cross-modal temporal attention among S given q, and then conduct top-k feature selection over X; as formulated:\\nlinear where g4 and gs projection layers for different are differentiable top-k selec- types of features, selector is spatial features of Topk tion function to choose the seg The top-k selection can be implemented by expand ments_ ing the Gumbel-Softmax   trick [18] or based on optimal [46] for ranking and sorting: transport  formulations In simply conduct Gumbel-Softmax sampling this paper; we Topk times with replacement to achieve top-k selection. Note that we sample the segments  with replacement be- question could only involve one cause, in some cases, the hope We the model learns t0 enhance the most segment: related segment in such cases by re-sampling it, instead of forcing it to select an irrelevant segment; as sampling with- replacement will do. See supplementfor more discus- out The output of the module is sion about Top-k selection: xTxNxD {wklk: € B} € RTopk Xt where B is the set of = Topk segments selected indexes_\\nKey Figure 4 components of Iterative Spatial-Temporal Atten- tion Layer:   Since region selection follows the same architecture segment selection, we only show its inputs and outputs_ as\\nalong the temporal dimension: fk, fl ff). sk: pooll ( question feature is similarly obtained by pooling the The I(W1;- The pooling func- word features, q pool( WM ). tions can be chosen from mean pooling, first token pooling; specific type of simple MLP layer; etc. according to the vision-language Transformer: For example, for image- used language Transformers, like CLIP; the first token pooling extracting frame and question features and can be used for obtaining segment features. pooling over frames for mean\\nRegion Selection: For the T-th sampled frame, we want to select its most relevant patches with the question. Given its region feature of one frame Xv {ck, In € [1, N],k € along with question q, perform cross-model atten- 3 we\\nGiven the segment features S patch features k=l\\'\\nExperiments 4.\\ntion over spatial patches of the T-th sampled frame and se- most related patches. Topj lect the This can be formulated as:\\n4.1. Datasets\\nWe model four recently   proposed evaluate our on challenging datasets for the long-form VideoQA, namely AGQA 14], NExT-QA 44], STAR 42] and Env-QA [11].\\nAGQA is an open-ended VideoQA benchmark for com positional spatio-temporal reasoning: We use its v2 version a more balanced distribution, as the dataset cre- which has It provides 2.27M QA pairs over 9.7K ator recommended. videos with an average length of 30 seconds NExT QA a multi-choice VideoQA benchmark for causal and tem- is reasoning: It contains a total of 5.4K videos   with poral an average length of 44s and about 52K questions STAR is another multi-choice VideoQA benchmark for Situated Reasoning: clips STAR contains 22K video with an aver- age length of 12s along with 60K questions Env-QA is an open-ended VideoQA benchmark for dynamic environment understanding: It contains 23K egocentric videos with an average length of 20 seconds collected on virtual environ- ment AIZTHOR [21] along with 8SK questions.\\nembedding layers for linear feature where hg and hx are projection: The output of the given each frame is XT RTop;xD_ Finally, we stack the selected patch features of {Xclr € [1,Topk all selected frames t0 obtain Xst Tl\\nSpatial-Temporal  Self-Attention. Given the selected frames and selected regions, along with the question, we self-attention layer to reason out fused aim to employ feature vector to jointly represent the question and video_\\nRegarding the inputs of self-attention, since the main computation cost comes from too many patches (K xTx N about thousands of patches), we only keep the selected ones_ While for temporal information, we keep all segments as the total number is only K (usually less than 10), which doesn bring heavy cost and can benefit more comprehensive multi- event reasoning\\nFor each benchmark we follow standard protocols out- works [1,5, 11, 14] for dataset processing; lined by prior metrics, and settings. Please see supplementfor details.\\nSpecifically, we first add type embedding to indicate the types feature, e.g- image region, segment Or word. The type embedding is formulated Ph € {Dh(h)lh € [1,3]} to each trainable embedding indicating where Oh is feature for layer: Then, a standard multi-head attention is performed to obtain the contextual features of all input tokens, formulated as:\\n4.2. Implementation Details\\nOur proposed method can be built upon most of the pre- trained multi-modal Transformers. In OUr experiments, we typical types of pre-trained models, CLIP Vit try two B/32) [37] for image-language pre-training models and All- 40] for video-language pre-training in-One-Base model denoted as MIST -CLIP and MIST -AIO respectively. In MIST Topj Topk 2 and 12 in cascade selec we set tion module and the layer of ISTA L For all videos_ 2_ we sample 32 frames per video, and split them into K AdamW is utilized to optimize model training: segments. Our model is trained on NVIDIA RTX A5O00 GPUs and implemented in PyTorch:\\nDx, and Dw are linear transformation where Qs\"\\nTay- Iterative Execution of ISTA: A stack of L ISTA modelling multi-event interactions between a ers is used for given question and video, where the updated segment fea- tures and word features are fed into next layer: The output of each layer {XO)}L_ prediction. is used for answer\\n4.3. Comparison with State-of-the-arts\\n3.3. Answer Prediction\\nWe compare our model with the state-of-the-art (SOTA_ VideoQA datasets (i.e-, AGQA v2, NExT; methods on four STAR, and Env-QA), as shown in Tab. and 4 re- 3 2, 13 We can see that our proposed method achieves spectively: state-of-the-art performances and outperforms the existing methods on all datasets. The performance relatively is gain limited on Env-QA, because its videos are recorded in virtual environment, AIZTHOR: There is a domain gap for CLIP feature, while previous SOTA uses the features pre- trained on virtual environment data.\\nSpecifically, we mean pool the token features of all ISTA X(L) ). MeanPool(XOl layers, Xo In addition, fol- lowing the work [49], we calculate the similarity between the Xo and the feature of all candidate answers XA € A} obtained by using the pre-trained model Fi- {xa/a nally, the candidate answer with the maximal similarity is considered as the final prediction y_\\nNotably, among SOTAs, TEMP[ATP] [5] uses the same MIST -CLIP feature, CLIP (ViT-B/32), And All-in- as MIST -AIO also use the same feature, 40] and All- one\\nlosst During training; we optimize the softmax cross-entropy between the predicted similarity scores and ground truth\\nTable 1. QA accuracies of state-of-the-art (SOTA methods on AGQA v2 test set\\nQA accuracies of variants of MIST on AGQA v2 and Table 5. NExT-QA\\nTable 2. QA accuracies of SOTA methods on NExT-QA val set\\nlarge passes these models with margin on questions requir- ing causality O multi-event reasoning, e.g,, Sequencing in Temporal in NExT-QA, Interaction AGQA v2, Causal & & Prediction in STAR, and Event in Env-QA These re - sults demonstrate that Our proposed model can effectively address the unique challenges of long-form video QA.\\n4.4. Comparison with Baselines\\nlong we devise several alternative solutions for Here form video modeling to replace our proposed ISTA . Specif- ically, in our CLIP-based MIST framework, we compare ISTA against other solutions, by fine-tuning the same pre- training input representation on AGQA v2 dataset:\\nTable 3. QA accuracies of SOTA methods on STAR val set:\\nMeanPool: It simply takes the average of frame features as the representation of the whole video_ seqTransf Trans.-Frame: We follow the in type CLIPAClip,   utilizing perform Transformer self- to attention over frame features t0 represent the video. Trans.-Patch: This model is similar t0 Trans.-Frame, but it performs self-attention over all patch tokens Divided STA: We follow TimeSformer [4] in the video to   perform classification uni-modal two-step model Space-Time Attention over image patches.\\nTable 4. QA accuracies of SOTA methods on Env-QA test set\\nin-One-Base. Compared to these methods, it can be found that our two versions of models, which build upon differ- ent types of pre-trained models, achieve substantial perfor- on all datasets_ gains mance\\nfrom the question type breakdown of each Moreover; dataset, compared that with AIO and we can see significant per- Temp[ATP], our model obtains a much more formance boost on questions that multi-grained vi- require sual concepts reasoning (i.e. AGQA Rel.-act , Obj.-act. on which  mainly  require information v2) than those within AGQA one frame (i.e;, Obj -rel v2 and Descriptive on on NExT-QA) In addition, we can see that ur model sur-\\nFrom the results in Tab. 5 we can see that ISTA achieves substantial improvement over other variants with larger than In addition, we 3% improvement on the overall accuracy: Transformer-based find that for long-form VideoQA the prediction models are much better than the Mean- answer Pool method while in the video-text retrieval field, some- times mean pooling is even better: The reason could be that long-form video is often complex and di- the content of\\nTable 6. Ablations results of ISTA on AGQA v2 and NExT-QA\\nsimple method for aggregating all frame fea- verse, and pooling, may cause information loss. tures, such as mean And long-form video QA requires more powerful temporal and spatial reasoning ability to focus O some details of a video, while mean pooling only performs well on capturing overall content.\\nMoreover we can see that it is helpful to consider region information in long-form QA (Divided STA and Trans. outperform But, neither dense self- Trans.-Frame) Path attention nor divided STA considers the interaction among multi-grained concepts; thus, the performance improvement is limited. And after integrating different granularities of vi- sual concepts during reasoning, our method benefits the per- All the above findings show that our method is formance. effective, and transferring pre-trained transformers to long form video QA is a challenging topic worth exploring:\\nFigure 5_ Performances of MIST with different settings. (a-e) Performances of MIST with different hyper-parameters on AGQA (f) Performance of variants of MIST under different GFLOPs v2_ on AGQA v2, where GFLOPs rise with the number of sampled frames increase\\n4.5. Ablation Study\\nIn this section, we propose several sets of variants MIST to show the effectiveness of its key components_\\ndoesn hurt performance too much The number of objects in the video frames is relatively small (compared with natu- ral scene images in image QA), and after temporal attention, the patch number has already been greatly reduced: So, the existing model is able to effectively focus O the appropri- mentioning that we can reduce objects. But; It is worth ate the computation cost by using a spatial selection module It may be useful when we face high-resolution or extremely complex videos in the future\\nkey - Effect of each component in ISTA . We ablate mod- ules in ISTA layer; i.e-, Segment  Selection, Region Se- lection, or   Self-attention   layer;  denoted MIST wlo as SSIRSISTA, respectively:\\nSS: It removes the Segment Selection mod- MIST wlo ule, and only performs region selection. Patch features with word features are fed into the self-attention module_ Segment Selection module_ MIST wlo_ RS: It removes All region features within selected segments are fed into self-attention layer: MIST wlo: STA: The segment features and selected region features are mean pooled as the output of ISTA\\nEffects of different ISTA configurations.  In this part, we try different configurations of model architecture, in- cluding Topk; a number of selected segments select patches Topj; ISTA layers L, and the number of segments K. The results are shown in Fig: 5 (a-d).\\nAGQA v2 and NExT-QA The results of these variants on We can see that removing Segment Se- are shown in Tab. 6. larger than 3% accuracy drop. The reason lection causes could be that removing it will introduce a lot of irrelevant re- gion information when predicting the answer and thus hurt the performance. 6 also shows that Segment Selection Tab: is important for multi-event reasoning because removing it hurts the performances on questions requiring temporal rea- soning; i.e., Causal and Temporal.\\nFirst, Fig: 5 (a) shows that the performance is relatively The performance slightly drops Topk. under the small pgood Topk The reason could be that large k if further increases_ will introduce either some incorrect segments O repeated Incorrect segments will bring misleading infor- segments. causing performance drops. Repeated segments lead mation larger number of repeated region features, causing it to difficult for the model to focus 0 question and segment Topj For the number of selected patches information: as shown in Fig: 5 (b), we can see that with the increase of performance first increases and then reaches sta- Topj, the bility: The reason for this phenomenon could be that when selecting too few image regions, it may incorrectly filter answering questions. some regions used for And when the\\ndrop In addition; the performance on both datasets is significant when removing Spatial-temporal self-attention The reason may be similar to MeanPool. We need a power- multi-grained reasoning: ful model to capture\\nthat  removing  spatial Moreover; attention see we can\\nFigure 6. Qualitative results of MIST on NExT-QA dataset We visualize its prediction results along with spatial-temporal attention where the frames with purple and red outlines indicate the highest temporal attention score in the first and second ISTA layers, respectively:\\n4.7. Qualitative Results\\nselected regions increase, though it introduces some irrele- regions, since the patch number after segment selection vant is already relatively small, the self-attention module can ef- fectively attend to relevant regions.\\nWe visualize some success and failure cases from the Fig: NExT-QA dataset in It can be seen that our model 6_ explicitly select video clips and image regions relevant can to the question. We can also find that it is difficult for the model to correctly select segments and regions, when the question mainly involves some concepts related to social Existing pre-trained models may not emotions_ well un derstand the correspondence between abstract concepts and videos. However; we believe that these issues can be allevi- ated by proposing better pre-trained models on short videos and our method is easy to build upon the stronger ones_\\nFor the number of ISTA layers, as shown in Fig: 5 (c) with the increase of L, the performance increases first and then reaches stability or slightly drops. It shows that stack- ing several layers of ISTA can benefit multi-event reason- In addition, the performance doesn t constantly in ing: larger L_ crease with This is probably due to (1) the datasets deeper network and (2) the large enough to train are not questions usually only involving two O three events, so con- bring sidering more benefits_ Fig: more events may not 5 (d) shows that when varying the number of video segments performance tends to suffer when the videos under- are segmentation, because, in this case, each segment spans relatively long duration, and hence the Segment Selection More importantly, all those findings im- module is useless_ ply that MIST is effective in multi-event reasoning by at- tending t0 multiple segments.\\n5. Conclusion and Future Work\\nThis   paper introduces Multi-modal   Iterative   Spatial- temporal Transformer for long-form VideoQA, which de- composes dense self-attention into a cascade segment and region selection module to increase the computation effi- ciency along with a self-attention layer to reason over vari- grained visual concepts. In addition, by iteratively con- ous ducting selection and attention layers; MIST better over performs multi-event reasoning: Experimental results on four VideoQA datasets show its effectiveness and advan- tages in efficiency and interpretability. For future work, al- though MIST has increased the number of sample frames the ability to capture high-frequency motion may still need In addition, patch features naturally have to be improved complex object-level reasoning: some limitations in Re- cently, there have been some pre-trained models for specifi- cally modeling actions and objects It may be interesting to try more types of pre-trained models Or even combine many reasoning: of them to achieve more general\\n4.6. Computation Efficiency\\nIn Fig: 5 (e), we can see that the accuracy increases sig- nificantly when  sampling more frames. It indicates that sampling more frames for long video QA tasks could be Though current datasets  don \\'t provide videos necessary. long or hours duration, such with several minutes videos likely to be encountered in real application scenarios. are Efficiency issues thus could be a more crucial consideration Fig: in such cases_ 5 (f), we compare GFLOPs vS. In aC- long- curacy for ourS other form video QA methods_ against It can be seen that the standard Transformer over patches is computationally expensive. The frame-based method is lightweight in computation, but its performance is limited. little extra  computation Our method requires only but achieves much better performance. It is also worth men- tioning that MIST doesn\\'t enlarge model size for higher ef- ficiency: Compared with other methods, it oly contains some extra shallow networks for spatial-temporal attention_\\nAcknowledgements.  This project is supported by the Na- Research  Foundation,  Singapore tional under its NRFF Award NRF-NRFFI3-2021-0008. The computational work for this article was partially performed 0n resources of the National Supercomputing Centre, Singapore.\\nReferences \\n Now the paper ends. \\nQuestion: What are some potential directions to further improve the proposed model to address its current limitations?',\n",
       " 'input': None,\n",
       " 'input_ids': tensor([128000,     58,  65562,  ...,   1646,     13, 128001]),\n",
       " 'labels': tensor([  -100,   -100,   -100,  ...,   1646,     13, 128001])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = data_processor.transform(data.take(200))\n",
    "tokenized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad7499c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [1, 2, 3]\n",
      "b [4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "d = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n",
    "for i in d:\n",
    "    print(i, d[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457fb8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.3423, -12.3423, -12.3423, -12.3423, -12.3423, -12.3423, -12.3423,\n",
       "        -12.3423, -12.3423, -12.3423])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor([float(-12.342314)]*10) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
